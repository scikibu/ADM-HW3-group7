{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import csv, sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the initial csv file and remove unnamed column\n",
    "df = pd.read_csv('Airbnb_Texas_Rentals.csv')\n",
    "df = df.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one tsv file per each row\n",
    "df = pd.read_csv('Airbnb_Texas_Rentals.csv')\n",
    "df = df.drop('Unnamed: 0', axis = 1)\n",
    "for i in range(df.index.max()+1):\n",
    "        new = open('doc_'+str(i)+'.tsv', 'w')\n",
    "        for j in range(9):\n",
    "               new.write('%s\\t' %df.iloc[i, j])\n",
    "        new.close()\n",
    "        if i > 2: # for the moment just making 4 files\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to remove stop words from specified column from data frame\n",
    "# in our case we'll call it for the title (column 4) and description (column 7)\n",
    "def remove_stopwords(df, n):\n",
    "    #remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    df[n] = df[n].apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to remove punctuation from specified column from data frame\n",
    "# in our case we'll call it for the title (column 4) and description (column 7)\n",
    "def remove_punctuation(df, n):\n",
    "    df[n] = df[n].str.replace('[^\\w\\s]','') \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to put everything to lowercase from specified column from data frame\n",
    "# in our case we'll call it for the title (column 4) and description (column 7)\n",
    "def to_lowercase(df, n):\n",
    "    df[n] = df[n].apply(lambda x: \" \".join(x.lower() for x in x.split())) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to do stemming from specified column from data frame\n",
    "# in our case we'll call it for the title (column 4) and description (column 7)\n",
    "def stemming(df, n):\n",
    "    st = PorterStemmer()\n",
    "    df[n] = df[n].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function which makes unique words from max 2 given columns from data frame\n",
    "# second argument can be omitted\n",
    "# !!!!! what shell we do with numbers? keep them as a word or remove them?\n",
    "\n",
    "def unique_words(df, n, m = None):\n",
    "    a=set(\" \".join(df[n]).split(\" \"))\n",
    "    if m == None:\n",
    "          b=set() # empty set for the second argument\n",
    "    else:\n",
    "        b=set(\" \".join(df[m]).split(\" \"))\n",
    "    # return unique words\n",
    "    return a.union(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_idx(inverted_idx, w, i):\n",
    "    for w in doc_unique_words:\n",
    "        inverted_idx[w].append(i)\n",
    "    return inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of all unique words appearing in documents\n",
    "vocab={}\n",
    "# word index of all unique words appearing in documents\n",
    "word_idx=defaultdict(list)\n",
    "\n",
    "# inverted index\n",
    "inverted_idx={}\n",
    "\n",
    "for i in range(0,3): #len(df.index)+1\n",
    "    #temoporaly create a dataframe from our tsv files\n",
    "    tsv = pd.read_csv('doc_{}.tsv'.format(i), sep='\\t', encoding = 'ISO-8859-1', header = None)\n",
    "    \n",
    "    # replacing \\n with the space\n",
    "    tsv = tsv.replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\\\n',  ' ', regex=True)\n",
    "    \n",
    "    # replacing / with the space\n",
    "    tsv = tsv.replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'/',  ' ', regex=True)\n",
    "    \n",
    "    # Data pre-processing\n",
    "    tsv=remove_stopwords(tsv, 4) # remove stop words in title\n",
    "    tsv=remove_stopwords(tsv, 7) # remove stop words in description\n",
    "    tsv=remove_punctuation(tsv, 4) # remove punctuation in title\n",
    "    tsv=remove_punctuation(tsv, 7) # remove punctuation in description\n",
    "    tsv=to_lowercase(tsv, 4) # put title into lower case\n",
    "    tsv=to_lowercase(tsv, 7) # put title into lower case\n",
    "    tsv=stemming(tsv, 4) # make stemming in title\n",
    "    tsv=stemming(tsv, 7) # make stemming in description\n",
    "    \n",
    "    # make a set of unique words from description and title of a document\n",
    "    doc_unique_words=unique_words(tsv,4,7)\n",
    "    \n",
    "    # update current vocabulary with the new words found in a document\n",
    "    for w in doc_unique_words:\n",
    "        if w in vocab.values(): # if word is already in general vocabulary skip it\n",
    "            continue\n",
    "        else:\n",
    "            if any(vocab)==False: #if vocabulary is empty then define initial integer key for the first word\n",
    "                v_idx=10000\n",
    "                vocab.update( { v_idx: w} ) # add a word to the vocabulary\n",
    "            else: # if vocabulary already has words, take the first available key (max(key)+1) for the next word\n",
    "                v_idx=max(list(vocab.keys()))+1\n",
    "                vocab.update( { v_idx: w} ) # add a word to the vocabulary\n",
    "                \n",
    "    # create index dictionary which for each word will have list of document numbers which contain that word\n",
    "    for w in doc_unique_words:\n",
    "        word_idx[w].append(i)\n",
    "word_idx=dict(word_idx)\n",
    "\n",
    "# Inverted index for all the words - dictionary with word_id as a key and list of the id of the documents that contain the word\n",
    "inverted_idx=dict((key,word_idx[value]) for (key, value) in vocab.items())\n",
    "\n",
    "with open('inverted_index.csv', 'w') as f:\n",
    "    w = csv.DictWriter(f, inverted_idx.keys())\n",
    "    w.writeheader()\n",
    "    w.writerow(inverted_idx)\n",
    "\n",
    "with open('vocabulary.csv', 'w') as f:\n",
    "    w = csv.DictWriter(f, vocab.keys())\n",
    "    w.writeheader()\n",
    "    w.writerow(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a beautiful house with garden and beach\n"
     ]
    }
   ],
   "source": [
    "query = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [str(query)]\n",
    "query = pd.DataFrame(query)\n",
    "query.replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\\\n',  ' ', regex=True)\n",
    "query.replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'/',  ' ', regex=True)\n",
    "query=remove_stopwords(query, 0) # remove stop words\n",
    "query=remove_punctuation(query, 0) # remove punctuation\n",
    "query=to_lowercase(query, 0) # put query into lower case\n",
    "query=stemming(query, 0) # make stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_set = unique_words(query, 0)\n",
    "vocabulary = pd.read_csv('vocabulary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vocabulary.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocabulary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
